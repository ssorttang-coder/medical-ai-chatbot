import { NextRequest, NextResponse } from 'next/server'

// ì‹¤ì œ OpenAI API ì—°ë™ì„ ìœ„í•œ ì„¤ì •
const OPENAI_API_KEY = process.env.OPENAI_API_KEY
const OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions'

console.log('API ë¼ìš°íŠ¸ ë¡œë“œë¨, OPENAI_API_KEY:', OPENAI_API_KEY ? 'ì„¤ì •ë¨' : 'ì„¤ì •ë˜ì§€ ì•ŠìŒ')

// ì˜ë£Œ íŠ¹í™” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì‚¬ ì—­í• 
const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ì˜ì‚¬ì…ë‹ˆë‹¤. í™˜ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ë©´ì„œ ì¦ìƒì„ ì²´ê³„ì ìœ¼ë¡œ íŒŒì•…í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ğŸ¯ ì—­í• ê³¼ íƒœë„:
- ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©
- í™˜ìì˜ ê°ì •ê³¼ ê±±ì •ì— ê³µê°í•˜ëŠ” íƒœë„
- ì „ë¬¸ì ì´ë©´ì„œë„ ë”°ëœ»í•œ í†¤ ìœ ì§€
- í™˜ìì˜ ë‹µë³€ì„ ì¶©ë¶„íˆ ë“£ê³  ë‹¤ìŒ ì§ˆë¬¸í•˜ê¸°

ğŸ“‹ ëŒ€í™” ì§„í–‰ ë°©ì‹:
1. í™˜ìì˜ ì¦ìƒì„ í•˜ë‚˜ì”© ì°¨ê·¼ì°¨ê·¼ íŒŒì•…
2. ê° ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê¸°ë‹¤ë¦° í›„ ë‹¤ìŒ ì§ˆë¬¸ ì§„í–‰
3. ì¦ìƒì˜ íŒ¨í„´ê³¼ ì›ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ íŒŒì•…
4. í•„ìš”ì‹œ ìœ„í—˜ ì‹ í˜¸ í™•ì¸
5. ì ì ˆí•œ ì¡°ì–¸ê³¼ ê¶Œê³ ì‚¬í•­ ì œì‹œ

ğŸ” ì •ë³´ ìˆ˜ì§‘ ìš°ì„ ìˆœìœ„:
1. ì£¼ìš” ì¦ìƒ (ì–´ë””ê°€ ì•„í”ˆì§€)
2. ë°œìƒ ì‹œê¸° (ì–¸ì œë¶€í„°)
3. ì¦ìƒ íŠ¹ì§• (ì–´ë–¤ ìƒí™©ì—ì„œ ì‹¬í•´ì§€ëŠ”ì§€)
4. í†µì¦ ê°•ë„ë‚˜ ì¶”ê°€ ì¦ìƒ
5. ì›ì¸ì´ë‚˜ ìœ ë°œ ìš”ì¸

âœ… ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ì˜ˆì‹œ:
í™˜ì: "ë‘í†µì´ ìˆì–´"
ì˜ì‚¬: "ë‘í†µì´ ì–¸ì œë¶€í„° ì‹œì‘ë˜ì…¨ë‚˜ìš”?"
í™˜ì: "ì–´ì œë¶€í„°"
ì˜ì‚¬: "ì–´ì œë¶€í„° ë‘í†µì´ ì§€ì†ë˜ê³  ìˆêµ°ìš”. ë‘í†µì´ íŠ¹íˆ ì–´ë–¤ ìƒí™©ì—ì„œ ì‹¬í•´ì§€ë‚˜ìš”?"

ğŸ“ ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œìš©:
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¤‘ë³µ ì§ˆë¬¸ ë°©ì§€
- ì´ë¯¸ ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ ê²°ì •
- í™˜ìê°€ ì–¸ê¸‰í•œ ì¦ìƒë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤
- ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ë©´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì§„í–‰

âŒ í”¼í•´ì•¼ í•  ê²ƒë“¤:
- ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë™ì‹œì— í•˜ì§€ ì•Šê¸°
- ì´ë¯¸ ë‹µë³€ë°›ì€ ë‚´ìš©ì„ ë‹¤ì‹œ ì§ˆë¬¸í•˜ì§€ ì•Šê¸°
- ë„ˆë¬´ ë³µì¡í•˜ê±°ë‚˜ ì „ë¬¸ì ì¸ ìš©ì–´ ì‚¬ìš©í•˜ì§€ ì•Šê¸°
- êµ¬ì²´ì ì¸ ì§„ë‹¨ì´ë‚˜ ì•½ë¬¼ ì²˜ë°©í•˜ì§€ ì•Šê¸°

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ì‘ê¸‰ ìƒí™© ì‹œ ì¦‰ì‹œ 119 ì—°ë½ ê¶Œê³ 
- ì˜ë£Œì§„ ìƒë‹´ì˜ ì¤‘ìš”ì„± ê°•ì¡°
- í™˜ìì˜ ì•ˆì „ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤

ë‹µë³€ í˜•ì‹:
- ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ
- í™˜ìì˜ ë‹µë³€ì— ëŒ€í•œ ì´í•´ì™€ ê³µê° í‘œí˜„
- ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ ìœ ì§€
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì ì ˆí•œ ì§ˆë¬¸ ìƒì„±

ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ í›„ ìš”ì•½:
ğŸ“‹ **í˜„ì¬ ìƒí™© ìš”ì•½**
- ì£¼ìš” ì¦ìƒ: [ì¦ìƒ ìš”ì•½]
- ë°œìƒ ì‹œê¸°: [ì–¸ì œë¶€í„°]
- ì¦ìƒ íŠ¹ì§•: [ì–´ë–¤ ìƒí™©ì—ì„œ ì‹¬í•´ì§€ëŠ”ì§€]

ğŸ” **ê°€ëŠ¥í•œ ì›ì¸**
- [ê°€ëŠ¥í•œ ì›ì¸ 1-2ê°œ]

âš ï¸ **ì£¼ì˜ì‚¬í•­**
- [ì¦‰ì‹œ ë³‘ì› ë°©ë¬¸ì´ í•„ìš”í•œ ê²½ìš°]
- [ì¼ë°˜ì ì¸ ì£¼ì˜ì‚¬í•­]

ğŸ’¡ **ê¶Œê³ ì‚¬í•­**
- [ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­]
- [ë³‘ì› ë°©ë¬¸ ê¶Œê³ ]

í•­ìƒ "ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤"ë¡œ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.`

// ì‘ê¸‰ ìƒí™© í‚¤ì›Œë“œ
const EMERGENCY_KEYWORDS = [
  'ì‹¬í•œ í†µì¦', 'ì˜ì‹ ìƒì‹¤', 'í˜¸í¡ ê³¤ë€', 'ì¶œí˜ˆ',
  'ê°€ìŠ´ í†µì¦', 'ë§ˆë¹„', 'ë°œì‘', 'ì¤‘ë…', 'ì‘ê¸‰', '119',
  'ì‹¬ì¥ë§ˆë¹„', 'ë‡Œì¡¸ì¤‘', 'ë³µí†µ', 'ê³ ì—´', 'ê²½ë ¨'
]

function isEmergency(message: string): boolean {
  return EMERGENCY_KEYWORDS.some(keyword => 
    message.toLowerCase().includes(keyword.toLowerCase())
  )
}

// ê°„ë‹¨í•œ ëŒ€í™” ë‹¨ê³„ ê²°ì •
function determineConversationStage(history: any[]): string {
  if (history.length === 0) return 'initial'
  
  const userMessages = history.filter(msg => msg.type === 'user')
  const messageCount = userMessages.length
  
  if (messageCount >= 6) return 'summary'
  if (messageCount >= 5) return 'detailed_analysis'
  if (messageCount >= 2) return 'symptom_collection'
  return 'initial'
}

// ëª¨ì˜ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (OpenAI API ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
function generateMockResponse(message: string, stage: string): string {
  const lowerMessage = message.toLowerCase()
  
  if (stage === 'summary') {
    return `ğŸ“‹ **í˜„ì¬ ìƒí™© ìš”ì•½**
- ì£¼ìš” ì¦ìƒ: í†µì¦
- ë°œìƒ ì‹œê¸°: ìµœê·¼
- ì¦ìƒ íŠ¹ì§•: ì§€ì†ì 

ğŸ” **ê°€ëŠ¥í•œ ì›ì¸**
- ë‹¤ì–‘í•œ ì›ì¸ ê°€ëŠ¥ì„±

âš ï¸ **ì£¼ì˜ì‚¬í•­**
- ì‹¬í•œ ì¦ìƒ ì‹œ ì¦‰ì‹œ ë³‘ì› ë°©ë¬¸

ğŸ’¡ **ê¶Œê³ ì‚¬í•­**
- ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ë³‘ì› ë°©ë¬¸ ê¶Œê³ 

ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.`
  }
  
  if (lowerMessage.includes('ë¨¸ë¦¬') || lowerMessage.includes('ë‘í†µ')) {
    return 'ë‘í†µì´ ì–¸ì œë¶€í„° ì‹œì‘ë˜ì…¨ë‚˜ìš”?'
  }
  if (lowerMessage.includes('ë°°') || lowerMessage.includes('ë³µí†µ')) {
    return 'ë³µí†µì´ ì–´ëŠ ìª½ì´ ë” ì•„í”„ì‹ ê°€ìš”?'
  }
  if (lowerMessage.includes('ê¸°ì¹¨')) {
    return 'ê¸°ì¹¨ì´ ì–¸ì œë¶€í„° ì‹œì‘ë˜ì—ˆë‚˜ìš”?'
  }
  if (lowerMessage.includes('ëª©')) {
    return 'ëª©ì´ ì–¸ì œë¶€í„° ì•„í”„ì‹ ê°€ìš”?'
  }
  
  return 'ì–´ë–¤ ì¦ìƒìœ¼ë¡œ ì˜¤ì…¨ë‚˜ìš”? ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.'
}

export async function POST(request: NextRequest) {
  try {
    console.log('POST ìš”ì²­ ë°›ìŒ')
    
    const { message, conversationHistory = [] } = await request.json()
    console.log('ìš”ì²­ ë°ì´í„°:', { message, conversationHistoryLength: conversationHistory.length })

    if (!message) {
      console.log('ë©”ì‹œì§€ê°€ ì—†ìŒ')
      return NextResponse.json(
        { error: 'ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 400 }
      )
    }

    // ì‘ê¸‰ ìƒí™© ê°ì§€
    const isEmergencyCase = isEmergency(message)
    console.log('ì‘ê¸‰ ìƒí™© ì—¬ë¶€:', isEmergencyCase)
    
    if (isEmergencyCase) {
      console.log('ì‘ê¸‰ ìƒí™© ê°ì§€ë¨')
      return NextResponse.json({
        response: `âš ï¸ ì‘ê¸‰ ìƒí™©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!

ì¦‰ì‹œ 119ì— ì—°ë½í•˜ê±°ë‚˜ ê°€ê¹Œìš´ ì‘ê¸‰ì‹¤ì„ ë°©ë¬¸í•˜ì„¸ìš”.

ì´ê²ƒì€ AI ìƒë‹´ì˜ í•œê³„ì´ë©°, ì „ë¬¸ ì˜ë£Œì§„ì˜ ì¦‰ì‹œ ì§„ë£Œê°€ í•„ìš”í•©ë‹ˆë‹¤.

ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ì˜ë£Œì§„ê³¼ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.`,
        isEmergency: true
      })
    }

    // OpenAI API í‚¤ í™•ì¸
    console.log('OpenAI API í‚¤ í™•ì¸:', OPENAI_API_KEY ? 'ìˆìŒ' : 'ì—†ìŒ')
    if (!OPENAI_API_KEY) {
      console.log('OpenAI API í‚¤ê°€ ì—†ìŒ')
      return NextResponse.json({
        error: 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env.local íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.'
      }, { status: 500 })
    }

    // ëŒ€í™” ë‹¨ê³„ ê²°ì •
    const conversationStage = determineConversationStage(conversationHistory)
    console.log('ëŒ€í™” ë‹¨ê³„:', conversationStage)

    // ëŒ€í™” íˆìŠ¤í† ë¦¬ ì •ë¦¬ (ìµœê·¼ 30ê°œ ë©”ì‹œì§€ë¡œ ì¦ê°€)
    const cleanedHistory = conversationHistory
      .filter((msg: any) => msg.content && msg.content.trim() !== '')
      .slice(-30) // 20ê°œì—ì„œ 30ê°œë¡œ ì¦ê°€

    // OpenAI API í˜¸ì¶œ
    const messages = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...cleanedHistory.map((msg: any) => ({
        role: msg.type === 'user' ? 'user' : 'assistant',
        content: msg.content
      })),
      { role: 'user', content: message }
    ]

    console.log('OpenAI API í˜¸ì¶œ ì‹œì‘:', { message, conversationStage, messagesCount: messages.length })

    // í† í° ì„¤ì • - ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¦ê°€
    const modelToUse = 'gpt-3.5-turbo-16k'
    let maxTokens = 1000 // 800ì—ì„œ 1000ìœ¼ë¡œ ì¦ê°€
    if (conversationStage === 'summary') {
      maxTokens = 1500 // 1200ì—ì„œ 1500ìœ¼ë¡œ ì¦ê°€
    }

    const response = await fetch(OPENAI_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${OPENAI_API_KEY}`
      },
      body: JSON.stringify({
        model: modelToUse,
        messages,
        max_tokens: maxTokens,
        temperature: 0.7
      })
    })

    console.log('OpenAI API ì‘ë‹µ ìƒíƒœ:', response.status)

    if (!response.ok) {
      const errorData = await response.text()
      console.error('OpenAI API ì˜¤ë¥˜:', errorData)
      
      // í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ë” ì €ë ´í•œ ëª¨ë¸ë¡œ ì¬ì‹œë„
      if (response.status === 429) {
        console.log('API í• ë‹¹ëŸ‰ ì´ˆê³¼, ë” ì €ë ´í•œ ëª¨ë¸ë¡œ ì¬ì‹œë„')
        
        try {
          const retryResponse = await fetch(OPENAI_API_URL, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${OPENAI_API_KEY}`
            },
            body: JSON.stringify({
              model: 'gpt-3.5-turbo',
              messages,
              max_tokens: 1000, // 800ì—ì„œ 1000ìœ¼ë¡œ ì¦ê°€
              temperature: 0.7
            })
          })
          
          if (retryResponse.ok) {
            const retryData = await retryResponse.json()
            const aiResponse = retryData.choices[0].message.content
            console.log('ì¬ì‹œë„ ì„±ê³µ, AI ì‘ë‹µ ìƒì„±ë¨:', aiResponse.substring(0, 100) + '...')
            
            return NextResponse.json({
              response: aiResponse,
              isEmergency: false,
              stage: conversationStage,
              isRetry: true
            })
          }
        } catch (retryError) {
          console.log('ì¬ì‹œë„ ì‹¤íŒ¨, ëª¨ì˜ ì‘ë‹µ ì‚¬ìš©')
        }
        
        // ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ëª¨ì˜ ì‘ë‹µ ì‚¬ìš©
        const mockResponse = generateMockResponse(message, conversationStage)
        return NextResponse.json({
          response: mockResponse,
          isEmergency: false,
          stage: conversationStage,
          isMock: true
        })
      }
      
      throw new Error(`OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: ${response.status} - ${errorData}`)
    }

    const data = await response.json()
    console.log('OpenAI API ì‘ë‹µ ë°ì´í„° ìˆ˜ì‹ ë¨')
    
    const aiResponse = data.choices[0].message.content
    console.log('AI ì‘ë‹µ ìƒì„±ë¨:', aiResponse.substring(0, 100) + '...')

    return NextResponse.json({
      response: aiResponse,
      isEmergency: false,
      stage: conversationStage
    })

  } catch (error) {
    console.error('Chat API Error:', error)
    const errorMessage = error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
    return NextResponse.json(
      { error: `ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${errorMessage}` },
      { status: 500 }
    )
  }
} 